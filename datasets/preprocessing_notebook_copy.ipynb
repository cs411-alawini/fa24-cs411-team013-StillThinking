{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kL0MRQIEw3dn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import hashlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KssmXs06tmqE",
        "outputId": "7cbf90d0-58ca-4230-b46c-8707751955c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-30.8.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Downloading Faker-30.8.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-30.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install faker\n",
        "from faker import Faker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHUAd05szopp"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NxqK0Tb6xB_B"
      },
      "outputs": [],
      "source": [
        "def preprocess_postings(input_path, output_path):\n",
        "\n",
        "    df = pd.read_csv(input_path, nrows=5000)\n",
        "    # Select only the columns we need based on SQL schema\n",
        "    needed_columns = [\n",
        "        'title',\n",
        "        'description',\n",
        "        'job_id',\n",
        "        'company_id'\n",
        "    ]\n",
        "\n",
        "    # Filter columns\n",
        "    df = df[needed_columns]\n",
        "\n",
        "    # Clean data\n",
        "    df = df.dropna(subset=['job_id', 'company_id'])  # Remove rows with missing critical data\n",
        "    df['title'] = df['title'].str[:10]  # Truncate to VARCHAR(10)\n",
        "    df['description'] = df['description'].str[:2048]  # Truncate to VARCHAR(2048)\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset=['title', 'job_id', 'company_id'])\n",
        "\n",
        "    df.insert(0, 'posting_id', range(1, len(df) + 1))\n",
        "\n",
        "    df = df.rename(columns={\n",
        "            'title':'posting_title',\n",
        "            'description':'posting_description',\n",
        "            'job_id':'job_id',\n",
        "            'company_id':'company_id'\n",
        "    })\n",
        "\n",
        "    # Save to new CSV\n",
        "    df.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2n5-PQLMp-UO"
      },
      "outputs": [],
      "source": [
        "def preprocess_jobs(input_path, output_path):\n",
        "    \"\"\"\n",
        "    Process jobs data according to SQL schema\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(input_path, nrows=5000)\n",
        "\n",
        "    # Select/rename columns based on salary information in your CSV\n",
        "    df = df[['job_id', 'max_salary', 'med_salary']]\n",
        "\n",
        "    # Clean data\n",
        "    df = df.dropna(subset=['job_id'])\n",
        "\n",
        "    # Convert salary columns to float/real\n",
        "    df['max_salary'] = pd.to_numeric(df['max_salary'], errors='coerce')\n",
        "    df['med_salary'] = pd.to_numeric(df['med_salary'], errors='coerce')\n",
        "\n",
        "    # Remove duplicates on job_id since it's a primary key\n",
        "    df = df.drop_duplicates(subset=['job_id'])\n",
        "\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"\\nProcessed {len(df)} jobs\")\n",
        "    print(\"\\nSample of processed data:\")\n",
        "    print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0uQcYOmHxJqK"
      },
      "outputs": [],
      "source": [
        "def preprocess_companies(input_path, output_path):\n",
        "    \"\"\"\n",
        "    Preprocess the companies.csv file to match our SQL schema.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(input_path)\n",
        "\n",
        "    needed_columns = [\n",
        "        'company_id',\n",
        "        'name',\n",
        "        'description'\n",
        "    ]\n",
        "\n",
        "    df = df[needed_columns]\n",
        "    df['name'] = df['name'].str[:10]\n",
        "    df['description'] = df['description'].str[:2048]\n",
        "    df = df.dropna(subset=['company_id'])\n",
        "    df = df.drop_duplicates(subset=['company_id'])\n",
        "\n",
        "    df = df.rename(columns={\n",
        "    'company_id': 'company_id',\n",
        "    'name': 'company_name',\n",
        "    'description': 'company_description'\n",
        "    })\n",
        "\n",
        "    df.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od3aTfYoxO1z"
      },
      "outputs": [],
      "source": [
        "def preprocess_skills(input_path, output_path):\n",
        "    \"\"\"\n",
        "    Preprocess the skills.csv file to match our SQL schema.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(input_path)\n",
        "\n",
        "    df['embedding'] = np.random.random(size=len(df))\n",
        "    \n",
        "    # Convert embeddings to string representation (as a single REAL number for SQL)\n",
        "    # We'll use the sum of the vector components as a simple representation\n",
        "    \n",
        "    df['skill_abr'] = df['skill_abr'].str[:10]\n",
        "    df['skill_name'] = df['skill_name'].str[:50]\n",
        "    df = df.dropna()\n",
        "    df = df.drop_duplicates(subset=['skill_abr'])\n",
        "\n",
        "    df = df.rename(columns={\n",
        "    'skill_abr': 'skill_abbr',\n",
        "    'skill_name': 'skill_name',\n",
        "    'embedding': 'embedding'\n",
        "    })\n",
        "\n",
        "    df.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0H4MWw0xSs7",
        "outputId": "7e97ff05-0db2-4d73-fad5-544c5671cd8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-e3f5f1d9cdf2>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['name'] = df['name'].str[:10]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed 5000 jobs\n",
            "\n",
            "Sample of processed data:\n",
            "     job_id  max_salary  med_salary\n",
            "0    921716        20.0         NaN\n",
            "1   1829192        50.0         NaN\n",
            "2  10998357     65000.0         NaN\n",
            "3  23221523    175000.0         NaN\n",
            "4  35982263     80000.0         NaN\n",
            "Preprocessing complete!\n"
          ]
        }
      ],
      "source": [
        "# Create output directory if it doesn't exist\n",
        "output_dir = Path('preprocessed_data')\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Process each file\n",
        "preprocess_postings('postings.csv', output_dir / 'postings_processed.csv')\n",
        "preprocess_companies('companies.csv', output_dir / 'companies_processed.csv')\n",
        "preprocess_jobs('postings.csv', output_dir / 'jobs_processed.csv')\n",
        "#preprocess_skills('skills.csv', output_dir / 'skills_processed.csv')\n",
        "\n",
        "print(\"Preprocessing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "30fSRVIAixiC"
      },
      "outputs": [],
      "source": [
        "def generate_user_data(n_users=1000, output_path='preprocessed_data/users.csv'):\n",
        "    \"\"\"\n",
        "    Generate dummy user data and save to CSV.\n",
        "    Uses Faker to create realistic usernames and hashed passwords.\n",
        "    \"\"\"\n",
        "    fake = Faker()\n",
        "\n",
        "    # Generate unique usernames\n",
        "    usernames = set()\n",
        "    while len(usernames) < n_users:\n",
        "        # Mix of formats: johnsmith, john.smith, john_smith93, etc.\n",
        "        username = fake.user_name()\n",
        "        if len(username) <= 20:  # Respect VARCHAR(20)\n",
        "            usernames.add(username)\n",
        "\n",
        "    # Convert to list and generate corresponding data\n",
        "    users_data = {\n",
        "        'user_id': range(1, n_users + 1),\n",
        "        'username': list(usernames),\n",
        "        'password': [hashlib.md5(fake.password().encode()).hexdigest() for _ in range(n_users)]  # Using MD5 for example purposes\n",
        "    }\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(users_data)\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    Path(output_path).parent.mkdir(exist_ok=True)\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HPxp5MoDtP1o"
      },
      "outputs": [],
      "source": [
        "def generate_user_skills(n_relations=2000, users_file='preprocessed_data/users.csv',\n",
        "                        skills_file='skills.csv',\n",
        "                        output_path='preprocessed_data/user_skills.csv'):\n",
        "    \"\"\"\n",
        "    Generate dummy user_skills relations between users and skills.\n",
        "    Each user will have 1-5 random skills.\n",
        "    \"\"\"\n",
        "    # Read users and skills\n",
        "    users_df = pd.read_csv(users_file)\n",
        "    skills_df = pd.read_csv(skills_file)\n",
        "\n",
        "    # Generate random relationships\n",
        "    relations = []\n",
        "    for user_id in users_df['user_id']:\n",
        "        # Randomly assign 1-5 skills to each user\n",
        "        n_skills = np.random.randint(1, 6)\n",
        "        skills = np.random.choice(skills_df['skill_abr'], size=n_skills, replace=False)\n",
        "\n",
        "        for skill in skills:\n",
        "            relations.append({\n",
        "                'user_id': user_id,\n",
        "                'skill_abr': skill\n",
        "            })\n",
        "\n",
        "    # Create DataFrame and take only the first n_relations if specified\n",
        "    df = pd.DataFrame(relations).head(n_relations)\n",
        "\n",
        "    # Remove any duplicates to maintain primary key constraint\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "bxJSUntNtbZA"
      },
      "outputs": [],
      "source": [
        "generate_user_data(n_users=1000)\n",
        "generate_user_skills(n_relations=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSCKNtSxtcYS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
